{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to be a Bioinformagician part 1\n",
    "- Author: Olga Botvinnik\n",
    "- Date: 2018-10-09\n",
    "\n",
    "Abstract: Many computational problems have already been solved and yet hundreds of hours are lost to re-solving them. This series provides tips and tricks that solve common pain points in bioinformatics, using AWS, reading/writing CSVs, extracting data out of file names, and more.\n",
    "\n",
    "## Prerequisities\n",
    "1. Everything you do should be full screen. Use [divvy](http://mizage.com/divvy/) to make managing windows easier - ask IT for a license\n",
    "1. [anaconda python](https://www.anaconda.com/download/#macos) - Install the 3.x (e.g. 3.7) version if you don't have it already\n",
    "1. aws account - Ask Olga or James for one\n",
    "1. awscli - on the command line, `pip install awscli` after you install Anaconda Python\n",
    "1. aegea - on the command line, `conda install aegea` after you install Anaconda Python\n",
    "1. nbconda - on the command line, `conda install nbconda` after you install Anaconda Python\n",
    "1. reflow. Click this link: https://github.com/grailbio/reflow/releases/download/reflow0.6.8/reflow0.6.8.darwin.amd64 Then on the command line:\n",
    "    ```\n",
    "    cd ~/Downloads\n",
    "    chmod ugo+x reflow0.6.8.darwin.amd64\n",
    "    sudo mv reflow0.6.8.darwin.amd64 /usr/local/bin/reflow\n",
    "    ```\n",
    "    Now the command `reflow` should output a lot of stuff:\n",
    "    \n",
    "    ```\n",
    "     î‚° reflow\n",
    "    The reflow command helps users run Reflow programs, inspect their\n",
    "    outputs, and query their statuses.\n",
    "\n",
    "    The command comprises a set of subcommands; the list of supported\n",
    "    commands can be obtained by running\n",
    "\n",
    "        reflow -help\n",
    "\n",
    "    ... (more stuff) ...\n",
    "    ```\n",
    "    Then configure reflow, following the [Confluence entry on Reflow (https://czbiohub.atlassian.net/wiki/spaces/DS/pages/838205454/reflow) instructions for configuration:\n",
    "    ```\n",
    "    AWS_SDK_LOAD_CONFIG=1 reflow setup-ec2\n",
    "    AWS_SDK_LOAD_CONFIG=1 reflow setup-s3-repository czbiohub-reflow-quickstart-cache\n",
    "    AWS_SDK_LOAD_CONFIG=1 reflow setup-dynamodb-assoc czbiohub-reflow-quickstart\n",
    "    export AWS_REGION=us-west-2\n",
    "    ```\n",
    "1. Claim a folder within `s3://czbiohub-cupcakes/` with today's date and your username, e.g.:\n",
    "    ```\n",
    "    s3://czbiohub-cupcakes/2018-10-09/olgabot/\n",
    "    ```\n",
    "\n",
    "\n",
    "Highly recommended:\n",
    "- If you haven't seen it already, follow https://github.com/czbiohub/codonboarding\n",
    "- Especially \n",
    "    - install homebrew - it makes your life better for installing packages on mac\n",
    "    ```\n",
    "    /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
    "    ```\n",
    "    - install Oh My ZSH: https://ohmyz.sh/\n",
    "    - Install exa: https://the.exa.website/ - MUCH easier to install if you installed homebrew.\n",
    "    ```\n",
    "    brew install exa\n",
    "    ```\n",
    "\n",
    "### Note: This notebook *can* be run locally \n",
    "\n",
    "But there's some weird version issues with conda packages on Mac and anyway it's cooler to launch from AWS so that's what we're doing\n",
    "\n",
    "## Installation\n",
    "\n",
    "### 1. Clone the cupcakes repo to `~/code` if you haven't already\n",
    "\n",
    "\n",
    "```\n",
    "mkdir ~/code\n",
    "cd ~/code\n",
    "git clone https://github.com/czbiohub/cupcakes\n",
    "```\n",
    "\n",
    "### 2. Create a \"bioinformagician\" environment\n",
    "\n",
    "```\n",
    "conda env create --name bioinformagician --file cupcakes/2018/olgas_bioinformagician_tricks/environment_no_versions_no_ng.yml\n",
    "```\n",
    "\n",
    "Then activate the environment\n",
    "\n",
    "```\n",
    "source activate bioinformagician\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are ready to read the data!\n",
    "\n",
    "Run each cell below by pressing Shift+Enter\n",
    "\n",
    "\n",
    "1. Helpful jupter notebook keystrokes: \n",
    "    - Ctrl-M-A add cell above\n",
    "    - Ctrl-M-B add cell below\n",
    "    - Ctrl-M-d d delete cell\n",
    "    - Ctrl-M-i interrupt\n",
    "    - Ctrl-M-0 restart\n",
    "\n",
    "The ones above are the shortcuts I use the most. Go to Help > Keyboard Shortcuts to see them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard convention is to import python standard libraries first, then third-party libraries after that\n",
    "# See list of standard libraries here: https://docs.python.org/3/library/\n",
    "# Both import lists should be alphabetically sorted\n",
    "\n",
    "# --- Python standard library --- #\n",
    "# Easily grab filenames from a folder\n",
    "import glob\n",
    "\n",
    "# Amazing library that I use almost every day for:\n",
    "# - chaining lists together into mega-lists\n",
    "# - \"multiplying\" lists against each other to get the full product of combinations\n",
    "import itertools\n",
    "\n",
    "# Read/write javascript object notation (JSON) files\n",
    "import json\n",
    "\n",
    "# Perform path manipulations\n",
    "import os\n",
    "\n",
    "# --- Third-party (non-standard Python) libraries --- #\n",
    "# python dataframes. very similar to R dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Make the number of characters allowed per column super big since our filenames are long\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read csv of lung cancer fastqs for which a kmer signature was calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5054, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>read1</th>\n",
       "      <th>read2</th>\n",
       "      <th>name</th>\n",
       "      <th>output</th>\n",
       "      <th>trim_low_abundance_kmers</th>\n",
       "      <th>dna</th>\n",
       "      <th>protein</th>\n",
       "      <th>ksizes</th>\n",
       "      <th>scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A10_B000419_S34</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B000419_S34/A10_B000419_S34_R1_001.fastq.gz</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B000419_S34/A10_B000419_S34_R2_001.fastq.gz</td>\n",
       "      <td>A10_B000419_S34</td>\n",
       "      <td>s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B000419_S34.signature</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>21,27,33,51</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A10_B000420_S82</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B000420_S82/A10_B000420_S82_R1_001.fastq.gz</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B000420_S82/A10_B000420_S82_R2_001.fastq.gz</td>\n",
       "      <td>A10_B000420_S82</td>\n",
       "      <td>s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B000420_S82.signature</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>21,27,33,51</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A10_B002073_S166</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002073_S166/A10_B002073_S166_R1_001.fastq.gz</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002073_S166/A10_B002073_S166_R2_001.fastq.gz</td>\n",
       "      <td>A10_B002073_S166</td>\n",
       "      <td>s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B002073_S166.signature</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>21,27,33,51</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A10_B002078_S202</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002078_S202/A10_B002078_S202_R1_001.fastq.gz</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002078_S202/A10_B002078_S202_R2_001.fastq.gz</td>\n",
       "      <td>A10_B002078_S202</td>\n",
       "      <td>s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B002078_S202.signature</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>21,27,33,51</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A10_B002095_S118</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002095_S118/A10_B002095_S118_R1_001.fastq.gz</td>\n",
       "      <td>s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002095_S118/A10_B002095_S118_R2_001.fastq.gz</td>\n",
       "      <td>A10_B002095_S118</td>\n",
       "      <td>s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B002095_S118.signature</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>21,27,33,51</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  \\\n",
       "0   A10_B000419_S34   \n",
       "1   A10_B000420_S82   \n",
       "2  A10_B002073_S166   \n",
       "3  A10_B002078_S202   \n",
       "4  A10_B002095_S118   \n",
       "\n",
       "                                                                                                                 read1  \\\n",
       "0    s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B000419_S34/A10_B000419_S34_R1_001.fastq.gz   \n",
       "1    s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B000420_S82/A10_B000420_S82_R1_001.fastq.gz   \n",
       "2  s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002073_S166/A10_B002073_S166_R1_001.fastq.gz   \n",
       "3  s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002078_S202/A10_B002078_S202_R1_001.fastq.gz   \n",
       "4  s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002095_S118/A10_B002095_S118_R1_001.fastq.gz   \n",
       "\n",
       "                                                                                                                 read2  \\\n",
       "0    s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B000419_S34/A10_B000419_S34_R2_001.fastq.gz   \n",
       "1    s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B000420_S82/A10_B000420_S82_R2_001.fastq.gz   \n",
       "2  s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002073_S166/A10_B002073_S166_R2_001.fastq.gz   \n",
       "3  s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002078_S202/A10_B002078_S202_R2_001.fastq.gz   \n",
       "4  s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/A10_B002095_S118/A10_B002095_S118_R2_001.fastq.gz   \n",
       "\n",
       "               name  \\\n",
       "0   A10_B000419_S34   \n",
       "1   A10_B000420_S82   \n",
       "2  A10_B002073_S166   \n",
       "3  A10_B002078_S202   \n",
       "4  A10_B002095_S118   \n",
       "\n",
       "                                                                 output  \\\n",
       "0   s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B000419_S34.signature   \n",
       "1   s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B000420_S82.signature   \n",
       "2  s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B002073_S166.signature   \n",
       "3  s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B002078_S202.signature   \n",
       "4  s3://olgabot-maca/lung_cancer/sourmash_v4/A10_B002095_S118.signature   \n",
       "\n",
       "   trim_low_abundance_kmers   dna  protein       ksizes  scaled  \n",
       "0                      True  True     True  21,27,33,51    1000  \n",
       "1                      True  True     True  21,27,33,51    1000  \n",
       "2                      True  True     True  21,27,33,51    1000  \n",
       "3                      True  True     True  21,27,33,51    1000  \n",
       "4                      True  True     True  21,27,33,51    1000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_samples = pd.read_csv('lung-cancer/compute/samples.csv')\n",
    "print(compute_samples.shape)\n",
    "compute_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the documentation of the `sourmash_search.rf` file to see what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\r\n",
      "\r\n",
      "val signature string (required)\r\n",
      "    S3 path to single signature file e.g.\r\n",
      "    s3://olgabot-maca/facs/sourmash_compute_all/A1-B000610-3_56_F-1-1.sig\r\n",
      "val database string (required)\r\n",
      "    S3 full path to the sourmash database folder containing the database folder e.g.\r\n",
      "    s3://olgabot-maca/facs/sourmash_index_all/tabula-muris-k21-protein/tabula-muris-k21-protein/\r\n",
      "    Note: this folder contains tabula-muris-k21-protein.sbt.json and a bunch of\r\n",
      "    hidden files\r\n",
      "val database_name string (required)\r\n",
      "    Name of the database e.g.: tabula-muris-k21-protein\r\n",
      "val output string (required)\r\n",
      "    CSV file to write with search results e.g\r\n",
      "    s3://olgabot-maca/facs/sourmash_search/A1-B000610-3_56_F-1-1_tabula-muris-k21-protein.csv\r\n",
      "val ksize int = 21\r\n",
      "    Size of kmer to use (can only use one for index)\r\n",
      "val sequence_to_compare string = \"dna\"\r\n",
      "    What to compare, could be either \"protein\" or \"dna\"\r\n",
      "val ignore_abundance bool = false\r\n",
      "    Whether or not to include the abundance of kmers in the comparison\r\n",
      "\r\n",
      "Declarations\r\n",
      "\r\n",
      "val Search func(signature file, ksize int, database dir, database_name string) (csv file)\r\n",
      "\r\n",
      "val Main unit\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! reflow doc reflow/sourmash_search.rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating a *Reflow batch job* which will submit 100s of lung cancer signatures to be looked up in the tabula muris database.\n",
    "\n",
    "\n",
    "For every argument, we'll need to create a column in a CSV that has that title, e.g. here we need the columns:\n",
    "\n",
    "- signature\n",
    "- database\n",
    "- database_name\n",
    "- output\n",
    "- ksize\n",
    "- sequence_to_compare\n",
    "- ignore_abundance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at aws folder of tabula muris kmer signature databases and save aws output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 's3://olgabot-maca/facs/sourmash_index_all'\n",
    "txt = 'sourmash_databases.txt'\n",
    "\n",
    "! aws s3 ls $prefix/ > $txt\n",
    "! cat $txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that we now have a file called `sourmash_databases.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running each line one-by-one is left as an exercise to the reader :)\n",
    "\n",
    "- To uncomment each cell, put your cursor in it, then:\n",
    "    1. select-all with Command-A \n",
    "    2. uncomment with Command-/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases = pd.read_table(txt, delim_whitespace=True, header=None, names=['is_prefix', 'database_name'])\n",
    "# databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases['database_name'] = databases['database_name'].str.strip('/')\n",
    "# databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases = databases.drop('is_prefix', axis=1)\n",
    "# databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases['ksize'] = databases['database_name'].str.extract('k(\\d+)').astype(int)\n",
    "# databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases['sequence_to_compare'] = databases['database_name'].map(lambda x: x.split('-')[-1])\n",
    "# databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases['database'] = databases['database_name'].map(lambda x: f\"{prefix}/{x}/{x}/\")\n",
    "# databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases = databases.set_index('database_name')\n",
    "# databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the table with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = pd.read_table(txt, delim_whitespace=True, header=None, names=['is_prefix', 'database_name'])\n",
    "databases['database_name'] = databases['database_name'].str.strip('/')\n",
    "databases = databases.drop('is_prefix', axis=1)\n",
    "databases['ksize'] = databases['database_name'].str.extract('k(\\d+)').astype(int)\n",
    "databases['sequence_to_compare'] = databases['database_name'].map(lambda x: x.split('-')[-1])\n",
    "databases['database'] = databases['database_name'].map(lambda x: f\"{prefix}/{x}/{x}/\")\n",
    "databases = databases.set_index('database_name')\n",
    "databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only compare on protein databases\n",
    "\n",
    "Since we're mapping human signatures onto a mouse database, we want to only compare on the protein signatures since protein sequences are more conserved than nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_databases = databases.query('sequence_to_compare == \"protein\"')\n",
    "protein_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the cool part!\n",
    "\n",
    "## \"Multiply\" the samples x databases x ignore abundances to get every combination\n",
    "\n",
    "We want to map each human lung sample onto ALL FOUR databases, PLUS we want to try both `ignore_abundance=True` and `ignore_abundance=False` \n",
    "\n",
    "Use `product` from Python's [itertools](https://docs.python.org/3/library/itertools.html) which is my favorite standard library module. (Though [collections](https://docs.python.org/3/library/collections.html) is a close second)\n",
    "\n",
    "Remember that Python was designed as \"batteries included\" so if you're doing something like doing a ton of nested for loops, know that many people have done that in the past and have figured out better ways to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_abundances = True, False\n",
    "\n",
    "data = list(itertools.product(compute_samples['output'], ignore_abundances,\n",
    "                              protein_databases.index))\n",
    "\n",
    "samples = pd.DataFrame(data, columns=['signature', 'ignore_abundance', 'database_name'])\n",
    "print(samples.shape)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the `cell_id` from the signature filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['cell_id'] = samples['signature'].map(lambda x: os.path.basename(x).split('.')[0])\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an output location\n",
    "\n",
    "This is the full path to where we'll be storing the output csv files from the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following to the output location you chose! Should start with s3://czbiohub-cupcakes, \n",
    "# e.g. s3://czbiohub-cupcakes/2018-10-09/olgabot but use YOUR username! (not \"olgabot!\")\n",
    "output_prefix = ''\n",
    "\n",
    "samples['output'] = output_prefix + samples['database_name'] + '/' + samples['cell_id'] + '.csv'\n",
    "samples['output'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the protein database information to the samples\n",
    "We'll use the [`join()` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) to combine the `protein_databases` table with our `samples` table, so we have the database `ksize`, `sequence_to_compare` and full URL.\n",
    "\n",
    "[Here](https://chrisalbon.com/python/data_wrangling/pandas_join_merge_dataframe/) is a nice blog post showing very clear examples of merge, join, and concatenate with pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_databases = samples.join(protein_databases, on='database_name')\n",
    "print(samples_databases.shape)\n",
    "samples_databases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a unique id for each row\n",
    "\n",
    "Reflow will create a log file for every job, and if you have duplicate ids, then those logs will get overwritten, and it won't treat those jobs as unique. So you want to have UNIQUE ids for each row.\n",
    "\n",
    "We use the [`apply()` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) to run the same function on every *row* of the dataframe. If we hadn't specified `axis=1`, it would have tried to apply the function to every *column* (`axis=0`). I usually forget which one is which so I try the function on one side and if it doesn't work I know it's the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_databases['id'] = samples_databases.apply(lambda x: \n",
    "                              '{cell_id}_ignore-abundance={ignore_abundance}_{database_name}'.format(**x), \n",
    "                              axis=1)\n",
    "samples_databases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset to a few dozen cells\n",
    "We don't need to see the result of ALL human cells, which is ~5,000. We can just look at the output of a few to get a feel for how well it is working. Below are the sample ids that I chose, and we'll subset using the [`.query()` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html) and access the python variable with `@chosen_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_ids = ['C14_B003528_S62',\n",
    " 'D1_B003125_S25',\n",
    " 'E19_B003570_S199',\n",
    " 'F21_B000420_S213',\n",
    " 'G10_B003586_S142',\n",
    " 'G4_B003570_S232',\n",
    " 'G9_B003511_S57',\n",
    " 'H7_B003588_S211',\n",
    " 'I22_B002095_S22',\n",
    " 'I3_B003573_S63',\n",
    " 'J11_B003573_S95',\n",
    " 'J8_B003528_S224',\n",
    " 'K7_B002073_S103',\n",
    " 'L16_B003588_S16',\n",
    " 'L5_B003588_S5',\n",
    " 'M1_B000420_S61',\n",
    " 'M23_B002097_S251',\n",
    " 'N15_B000420_S99',\n",
    " 'O3_B003573_S207',\n",
    " 'P14_B000420_S146',\n",
    " 'P2_B003125_S14']\n",
    "\n",
    "samples_subset = samples_databases.query('cell_id in @chosen_ids')\n",
    "print(samples_subset.shape)\n",
    "samples_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove sample ID column and set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_no_cell_id = samples_subset.drop(columns=['cell_id'])\n",
    "samples_no_cell_id = samples_no_cell_id.set_index('id')\n",
    "print(samples_no_cell_id.shape)\n",
    "samples_no_cell_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a folder to save reflow workflow to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is our current directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'lung-cancer/search_protein_databases'\n",
    "! mkdir $folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check that the folder exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write reflow batch `config.json` and `samples.csv` file\n",
    "\n",
    "We'll run the program `sourmash_search.rf` which is in the `reflow` folder here. I recommend keeping your reflow scripts separate from their batch folders as you may use the same script across multiple folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \t{\n",
    "    # Since the folder we're writing to is relative to here as \"lung-cancer/search_protein_databases\"\n",
    "    # but the reflow folder is \"reflow/\" then we need to go up two directories with \"../..\"\n",
    "\t\"program\": \"../../reflow/sourmash_search.rf\",\n",
    "\t\"runs_file\": \"samples.csv\"\n",
    "\t}\n",
    "\n",
    "# Make sure the index (the ids!) are unique\n",
    "assert samples_no_sample_id.index.is_unique\n",
    "\n",
    "samples_no_sample_id.to_csv(f'{folder}/samples.csv', index=True)\n",
    "\n",
    "\n",
    "with open(f'{folder}/config.json', 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the contents of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lha $folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of lines  in the folder to make sure it's the same as our input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l $folder/samples.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running reflow jobs\n",
    "\n",
    "\n",
    "To run the reflow batch, go to the terminal and navigate to the `cupcakes/2018/olgas_bioinformagician_tricks/lung-cancer/search_protein_databases` directory. Once you're there, run this command:\n",
    "\n",
    "```\n",
    "reflow runbatch\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bioinformagician]",
   "language": "python",
   "name": "conda-env-bioinformagician-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
