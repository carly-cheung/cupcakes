{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to be a Bioinformagician part 2\n",
    "- Author: Olga Botvinnik\n",
    "- Date: 2018-10-09\n",
    "\n",
    "Abstract: Many computational problems have already been solved and yet hundreds of hours are lost to re-solving them. This series provides tips and tricks that solve common pain points in bioinformatics, using AWS, reading/writing CSVs, extracting data out of file names, and lots more!\n",
    "\n",
    "## Prerequisities\n",
    "1. Everything you do should be full screen. Use [divvy](http://mizage.com/divvy/) to make managing windows easier - ask IT for a license\n",
    "1. [anaconda python](https://www.anaconda.com/download/#macos) - Install the 3.x (e.g. 3.7) version if you don't have it already\n",
    "1. aws account - Ask Olga or James for one\n",
    "1. awscli - on the command line, `pip install awscli` after you install Anaconda Python\n",
    "1. aegea - on the command line, `conda install aegea` after you install Anaconda Python\n",
    "1. nbconda - on the command line, `conda install nbconda` after you install Anaconda Python\n",
    "1. reflow. Click this link: https://github.com/grailbio/reflow/releases/download/reflow0.6.8/reflow0.6.8.darwin.amd64 Then on the command line:\n",
    "    ```\n",
    "    cd ~/Downloads\n",
    "    chmod ugo+x reflow0.6.8.darwin.amd64\n",
    "    sudo mv reflow0.6.8.darwin.amd64 /usr/local/bin/reflow\n",
    "    ```\n",
    "    Now the command `reflow` should output a lot of stuff:\n",
    "    \n",
    "    ```\n",
    "      reflow\n",
    "    The reflow command helps users run Reflow programs, inspect their\n",
    "    outputs, and query their statuses.\n",
    "\n",
    "    The command comprises a set of subcommands; the list of supported\n",
    "    commands can be obtained by running\n",
    "\n",
    "        reflow -help\n",
    "\n",
    "    ... (more stuff) ...\n",
    "    ```\n",
    "    Then configure reflow, following the [Confluence entry on Reflow (https://czbiohub.atlassian.net/wiki/spaces/DS/pages/838205454/reflow) instructions for configuration:\n",
    "    ```\n",
    "    AWS_SDK_LOAD_CONFIG=1 reflow setup-ec2\n",
    "    AWS_SDK_LOAD_CONFIG=1 reflow setup-s3-repository czbiohub-reflow-quickstart-cache\n",
    "    AWS_SDK_LOAD_CONFIG=1 reflow setup-dynamodb-assoc czbiohub-reflow-quickstart\n",
    "    export AWS_REGION=us-west-2\n",
    "    ```\n",
    "1. Claim a folder within `s3://czbiohub-cupcakes/` with today's date and your username, e.g.:\n",
    "    ```\n",
    "    s3://czbiohub-cupcakes/2018-10-09/olgabot/\n",
    "    ```\n",
    "1. [GitHub](http://github.com) username and membership to [@czbiohub](https://github.com/czbiohub/) GitHub group.\n",
    "\n",
    "Highly recommended:\n",
    "- If you haven't seen it already, follow https://github.com/czbiohub/codonboarding\n",
    "- Especially \n",
    "    - install homebrew - it makes your life better for installing packages on mac\n",
    "    ```\n",
    "    /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
    "    ```\n",
    "    - install Oh My ZSH: https://ohmyz.sh/\n",
    "    - Install exa: https://the.exa.website/ - MUCH easier to install if you installed homebrew.\n",
    "    ```\n",
    "    brew install exa\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How to be a bioinformagician, part 2\n",
    "\n",
    "# Step 1: Launch Jupyter Notebooks from AWS\n",
    "\n",
    "## 1.01 Use an existing packer image\n",
    "\n",
    "### YOU can make your own packer image\n",
    "\n",
    "I get questions about \"how can I make my own image?\" and people don't realize it's much easier than they think!\n",
    "\n",
    "- You aren't beholden to what already exists there\n",
    "- If the README instructions suck, ask me or James questions and we'll try to answer them. We'll also probably tell you to fix the README for future people with the same question\n",
    "- Look at the [installation script for this \"cupcakes\" image](https://github.com/czbiohub/packer-images/blob/master/scripts/cupcakes.sh) - it's super simple. You've already used those commands on AWS so it's not a huge leap to make a packer image.\n",
    "\n",
    "Here's the `aegea launch` command. You'll want to change the `olgabot` in  `olgabot-cupcakes` to be your own name.\n",
    "\n",
    "\n",
    "```\n",
    "aegea launch --ami-tags Name=czbiohub-cupcakes -t t2.xlarge --security-groups='R/RStudio Server and JupyterHub' --iam-role S3fromEC2 --duration-hours 24 olgabot-cupcakes\n",
    "```\n",
    "\n",
    "We're using a tiny instance (`t2.xlarge` - see [all instance options](https://www.ec2instances.info/)) because it's cheap and has enough memory (16GiB) for what we need. It'll shut down automatically after 24 hours so make sure to `git add` and `git commit` your changes!!!\n",
    "\n",
    "\n",
    "\n",
    "#### Aegea launch errors\n",
    "If you're getting an error that looks like:\n",
    "\n",
    "```\n",
    " Mon  8 Oct - 10:31  ~ \n",
    "  aegea launch --ami-tags Name=czbiohub-jupyter -t t2.xlarge --security-groups='R/RStudio Server and JupyterHub' --iam-role S3fromEC2 --duration-hours 24 olgabot-cupcakes\n",
    "Traceback (most recent call last):\n",
    "  File \"/anaconda3/bin/aegea\", line 23, in <module>\n",
    "    aegea.main()\n",
    "  File \"/anaconda3/lib/python3.6/site-packages/aegea/__init__.py\", line 78, in main\n",
    "    result = parsed_args.entry_point(parsed_args)\n",
    "  File \"/anaconda3/lib/python3.6/site-packages/aegea/launch.py\", line 50, in launch\n",
    "    dns_zone = DNSZone(config.dns.get(\"private_zone\"))\n",
    "  File \"/anaconda3/lib/python3.6/site-packages/aegea/util/aws/__init__.py\", line 183, in __init__\n",
    "    raise AegeaException(msg.format(len(private_zones)))\n",
    "aegea.util.exceptions.AegeaException: Found 2 private DNS zones; unable to determine zone to use. Set the dns.private_zone key in Aegea config\n",
    "```\n",
    "\n",
    "\n",
    "*NOTE: I found this fix by SEARCHING slack for \"aegea dns\". If you're getting an error, it's likely many other people are, too, so SEARCH slack if you're not getting a response on #eng-support right away*\n",
    "\n",
    "You can do one of two things, one that will fix the problem forever or a quick fix that will only work once.\n",
    "\n",
    "\n",
    "##### Edit your aegea config file so it never happens again\n",
    "\n",
    "```\n",
    "echo \"dns:\\n  private_zone: aegea\" >> ~/.config/aegea/config.yml\n",
    "```\n",
    "\n",
    "##### Fix it just this one time\n",
    "\n",
    "Add `--no-dns` to your `aegea launch` command before the image name (last argument):\n",
    "\n",
    "```\n",
    "aegea launch --ami-tags Name=czbiohub-jupyter -t t2.xlarge --security-groups='R/RStudio Server and JupyterHub' --iam-role S3fromEC2 --duration-hours 24 --no-dns olgabot-cupcakes\n",
    "```\n",
    "\n",
    "\n",
    "## 1.02 Log into your instance\n",
    "\n",
    "\n",
    "```\n",
    "aegea ssh ubuntu@olgabot-cupcakes\n",
    "```\n",
    "\n",
    "\n",
    "## 1.03 Clone my [rcfiles](https://github.com/olgabot/rcfiles) repo and run the setup\n",
    "\n",
    "I run this one-liner for every single EC2 instance that I make. This way, every instance it has exactly my setup and all my favorite programs, color themes, commands, and aliases. Feel free to fork and edit to your own favorite programs, themes, aliases, etc.\n",
    "\n",
    "```\n",
    "git clone https://github.com/olgabot/rcfiles ~/rcfiles && cp ~/rcfiles/Makefile ~ && cd ~ && make\n",
    "```\n",
    "\n",
    "The double ampersand (`&&`) means to do the next command only if the previous command was successful, and is a nice way to string together a bunch of stuff in a row, though it *can* be a little unreadable..\n",
    "\n",
    "If you're curious what the file is happening, the money is in the [Makefile](https://github.com/olgabot/rcfiles/blob/master/Makefile).\n",
    "\n",
    "### If you get an error `recipe for target 'install' failed`\n",
    "\n",
    "Sometimes computers are annoying and they lock files to prevent other processes from using them, but they can get locked for too long. Sometimes when you install stuff, you get this error:\n",
    "\n",
    "\n",
    "```\n",
    "sudo: unable to resolve host olgabot-cupcakes.aegea\n",
    "E: Could not get lock /var/lib/dpkg/lock - open (11: Resource temporarily unavailable)\n",
    "E: Unable to lock the administration directory (/var/lib/dpkg/), is another process using it?\n",
    "Makefile:4: recipe for target 'install' failed\n",
    "make: *** [install] Error 100\n",
    "```\n",
    "\n",
    "To fix it, do \n",
    "```\n",
    "sudo rm -rf /var/lib/dpkg/lock\n",
    "```\n",
    "\n",
    "Then do the last command of the one-liner again, `make`:\n",
    "\n",
    "```\n",
    "make\n",
    "```\n",
    "\n",
    "\n",
    "## 1.04 If zsh didn't start, type `zsh`\n",
    "\n",
    "```\n",
    "zsh\n",
    "```\n",
    "\n",
    "## 1.05 Start Screen/Tmux\n",
    "This will keep Jupyter notebook running forever even if your network connection breaks\n",
    "\n",
    "Do one of:\n",
    "```\n",
    "screen\n",
    "```\n",
    "--- OR if you know `tmux` much better ---\n",
    "\n",
    "```\n",
    "tmux\n",
    "```\n",
    "\n",
    "\n",
    "You may need to type `zsh` again so it starts the Z shell.\n",
    "\n",
    "\n",
    "## 1.06 Launch jupyter notebook\n",
    "\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "## 1.07 Open another tab in your terminal with Command-T\n",
    "\n",
    "Multiple tabs >> (are much better than)  multiple windows because it's much easier to navigate between them\n",
    "\n",
    "- `Command-Shift-[` moves one tab to the left\n",
    "- `Command-Shift-]` moves one tab to the right\n",
    "\n",
    "\n",
    "## 1.08 Tunnel the notebook from AWS to your computer\n",
    "\n",
    "By default, Jupyter Notebook uses port 8888, and if you have other Jupyter Notebooks running locally, this causes conflict. By rebinding the port to `8899`, then we can have Jupyter Notebooks running both locally and on AWS.\n",
    "\n",
    "This is the command that binds the remote port `8888` to your local port `8899`.\n",
    "\n",
    "```\n",
    "aegea ssh ubuntu@olgabot-cupcakes -NL localhost:8899:localhost:8888 \n",
    "```\n",
    "\n",
    "## 1.09 Go to http://localhost:8899 on your laptop\n",
    "\n",
    "The password is the same as the InnerHub wifi password.\n",
    "\n",
    "\n",
    "You can also copy the `localhost:8888/?token=asdfasdfadsf` url and replace `8888` with `8899`.\n",
    "\n",
    "```\n",
    " Tue 16 Oct - 01:42  ~ \n",
    "  jupyter notebook\n",
    "[I 01:42:25.272 NotebookApp] Writing notebook server cookie secret to /run/user/1000/jupyter/notebook_cookie_secret\n",
    "[I 01:42:27.836 NotebookApp] Serving notebooks from local directory: /home/ubuntu\n",
    "[I 01:42:27.836 NotebookApp] The Jupyter Notebook is running at:\n",
    "[I 01:42:27.836 NotebookApp] http://localhost:8888/?token=3d81239d82940adfc38110d14c7fc07cb6b3b520ed956e49\n",
    "[I 01:42:27.836 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
    "[W 01:42:27.836 NotebookApp] No web browser found: could not locate runnable browser.\n",
    "[C 01:42:27.836 NotebookApp] \n",
    "    \n",
    "    Copy/paste this URL into your browser when you connect for the first time,\n",
    "    to login with a token:\n",
    "        http://localhost:8888/?token=3d81239d82940adfc38110d14c7fc07cb6b3b520ed956e49\n",
    "```\n",
    "\n",
    "## 1.10 Navigate to the cupcakes/2018 folder\n",
    "\n",
    "- Open `002_how_to_be_a_bioinformagician_part02.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard convention is to import python standard libraries first, then third-party libraries after that\n",
    "# See list of standard libraries here: https://docs.python.org/3/library/\n",
    "# Both import lists should be alphabetically sorted\n",
    "\n",
    "# --- Python standard library --- #\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# --- Third-party (non-standard Python) libraries --- #\n",
    "# Interactive visualizations\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# Commonly used library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numerical python - arrays, nans\n",
    "import numpy as np\n",
    "\n",
    "# python dataframes. very similar to R dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Make the number of characters allowed per column super big since our filenames are long\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "# Reminder to install pyarrow for parquet read/write\n",
    "import pyarrow\n",
    "\n",
    "# Reminder to install s3fs to read files from aws\n",
    "import s3fs\n",
    "\n",
    "# static visualizations. ggplot2-like\n",
    "import seaborn as sns\n",
    "\n",
    "# Nice status bar for 'for' loops\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Show figures inside the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Re-import libraries every time you run a cell. A lifesaver for active development!\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_samples = pd.read_csv('lung_cancer/compute/samples.csv')\n",
    "print(compute_samples.shape)\n",
    "compute_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot have latest version of `awscli` ... there is a bug\n",
    "\n",
    "\n",
    "    pip install awscli==1.15.83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the bucket that we wrote the files to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls s3://olgabot-maca/lung_cancer/sourmash_search/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `aws s3 ls` from within Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls s3://olgabot-maca/lung_cancer/sourmash_search/tabula-muris-k51-protein/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here we'll do the same thing but save the s3 location into the variable `prefix`, and save the output of the `aws s3 ls` to the text file `lung_cancer_sourmash_search.txt`, then use the command line program `cat` to con_*cat*_enate it to standard output so we can see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 's3://olgabot-maca/lung_cancer/sourmash_search/tabula-muris-k51-protein'\n",
    "txt = 'lung_cancer_sourmash_search.txt'\n",
    "\n",
    "! aws s3 ls $prefix/ > $txt\n",
    "! cat $txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the table we wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_s3_ls = pd.read_table(txt, \n",
    "                          delim_whitespace=True, header=None, \n",
    "                          names=['date', 'time', 'bytes', 'basename'])\n",
    "print(aws_s3_ls.shape)\n",
    "aws_s3_ls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract out the sample id and whether abundance was on or off in the search\n",
    "\n",
    "I **always** use [regex101.com](https://regex101.com/r/UGtu79/1) to test out the regular expression and visually inspect that it's doing the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '(?P<sample_id>[\\w]+)_ignore-abundance=(?P<ignore_abundance>True|False).csv'\n",
    "\n",
    "sample_id_abundance = aws_s3_ls.basename.str.extract(pattern)\n",
    "print(sample_id_abundance.shape)\n",
    "sample_id_abundance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at all sample ids, sorted. There should be two each - one with and without abundance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sample_id_abundance.sample_id.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the sample ids + abundance info and the `aws s3 ls` results\n",
    "\n",
    "Here, we're adding the `sample_id_abundace` to the right, like \"cbind\" you may have seen in other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_s3_ls_ids = pd.concat([aws_s3_ls, sample_id_abundance], axis=1)\n",
    "print(aws_s3_ls_ids.shape)\n",
    "aws_s3_ls_ids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one of the files *DIRECTLY FROM S3* liek omg omg omg\n",
    "\n",
    "Here we're demonstrating that we can read csvs (really any text files) **directly** from S3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv('s3://olgabot-maca/lung_cancer/sourmash_search/tabula-muris-k51-protein/D1_B003125_S25_ignore-abundance=False.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're doing the same thing, but using variables because we're good coders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = f'{prefix}/D1_B003125_S25_ignore-abundance=False.csv'\n",
    "print(\"reading:\", csv)\n",
    "df = pd.read_csv(csv)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we show the number of rows and columns of the dataframe with `print(df.shape)` and the first 5 rows with `df.head()`. You should do this EVERY single time you transform your dataframe, to make sure what you thought would happen actually happened. I've totally screwed myself over by not double checking at every step so make sure you save yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all the csvs and use `tqdm` to show how fast we're going\n",
    "\n",
    "[`tqdm`](https://github.com/tqdm/tqdm), meaning \"progress\" in Arabic, is a super useful tool that shows you how quickly your loop is going. From before, we can see that the size of the `aws_s3_ls` dataframe is 42 and we can watch as the iteration proceeds.  \n",
    "\n",
    "We'll also use the `%%time` magic to see how long it all takes. You can read more about the built-in Jupyter magics [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html). The key thing to note is that a single percent sign `%` indicates a magic that only operates on that line, e.g. the\n",
    "\n",
    "    %time df = pd.read_csv(csv)\n",
    "\n",
    "line, while `%%time` with two percent signs `%` will capture the time of the whole cell. Note that the double percent sign ones must be the FIRST line in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i, row in tqdm(aws_s3_ls_ids.iterrows()):\n",
    "    basename = row.basename\n",
    "    sample_id = basename.split()\n",
    "    csv = f'{prefix}/{basename}' \n",
    "    df = pd.read_csv(csv)\n",
    "    df['query_sample_id'] = row.sample_id\n",
    "    df['ignore_abundance'] = row.ignore_abundance\n",
    "    dfs.append(df)\n",
    "search_results = pd.concat(dfs, ignore_index=True)\n",
    "print(search_results.shape)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases, you get a nice-looking progress bar, but `aws_s3_ls_ids.iterrows()` returns an iterator, not a list so no-one knows the total size *a priori*.\n",
    "\n",
    "In the example below, you get the nice-looking bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(500)):\n",
    "    # If i is divisible by 100 with 0 remainder\n",
    "    if i % 100 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write `extract_metadata.py` file with utility functions that we'll use across notebooks\n",
    "\n",
    "Now we're going to write a bunch of functions. But because these functions are SO USEFUL we're probably going to need them in different Jupyter Notebooks. So we'll write a file called `extract_metadata.py` using the `%%file` magic, and then we can import it like we import any other package, with `import extract_metadata`.\n",
    "\n",
    "This is really useful because then you only need to change the code in one place! Rather than having to copy/paste the new function into each file, which is just calling for disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file extract_metadata.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def combine_cell_ontology_free_annotation(row):\n",
    "    if pd.notnull(row['free_annotation']):\n",
    "        return '{cell_ontology_class} ({free_annotation})'.format(**row)\n",
    "    else:\n",
    "        return row['cell_ontology_class']\n",
    "\n",
    "\n",
    "def extract_cell_metadata(name_column, pattern='(?P<column>\\w+):(?P<value>[\\w-]+)'):\n",
    "    expanded = name_column.str.extractall(pattern)\n",
    "    expanded_index = expanded.reset_index()\n",
    "    annotations = expanded_index.pivot(columns='column', values='value', index='level_0')\n",
    "    \n",
    "    # Convert \"nan\" strings to actual NaN objects\n",
    "    annotations[annotations == \"nan\"] = np.nan\n",
    "    annotations['cell_ontology_free_annotation'] = annotations.apply(\n",
    "        combine_cell_ontology_free_annotation, axis=1)\n",
    "    return annotations \n",
    "\n",
    "\n",
    "def to_key_value_pair(attribute):\n",
    "    if len(attribute) > 1:\n",
    "        try:\n",
    "            return attribute[0], int(attribute[1])\n",
    "        except ValueError:\n",
    "            return attribute[0], attribute[1] \n",
    "    else:\n",
    "        return 'comparison_sequence', attribute[0]\n",
    "\n",
    "\n",
    "def extract_experiment_metadata(basename):\n",
    "    key = basename.split('.csv')[0]\n",
    "    split = key.split('_')\n",
    "    attributes = [x.split('=') for x in split]\n",
    "    attributes = dict(to_key_value_pair(x) for x in attributes)\n",
    "    return key, attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the file we just wrote and use the `extract_cell_metadata` function\n",
    "\n",
    "Now we'll import our `extract_metadata.py` file with `import extract_metadata`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_metadata\n",
    "\n",
    "cell_metadata = extract_metadata.extract_cell_metadata(search_results.name)\n",
    "print(cell_metadata.shape)\n",
    "cell_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `join` the search results on the metadata\n",
    "\n",
    "Notice that the `index` (the `pandas` word for row names) of both are the same - they are the same size and have the same number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_metadata = search_results.join(cell_metadata)\n",
    "print(search_results_metadata.shape)\n",
    "search_results_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we do anything else... SAVE!\n",
    "\n",
    "We've read in all these files and transformed them a bit. Let's not lose our work!! So we'll write our \n",
    "\n",
    "Write the files directly to AWS. This is not as straightforward as just using a filename that starts with `s3://` so I wrote this utility function that we can use across notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file s3_utils.py\n",
    "import s3fs\n",
    "\n",
    "def write_s3(df, filename, fmt='csv', **kwargs):\n",
    "    fs = s3fs.S3FileSystem(anon=False)\n",
    "    if fmt == 'csv':\n",
    "        # csv is a text format\n",
    "        with fs.open(filename, 'w') as f:\n",
    "            return df.to_csv(f, **kwargs)\n",
    "    elif fmt == 'parquet':\n",
    "        # Parquet is a binary format and needs the \"b\" flag\n",
    "        with fs.open(filename, 'wb') as f:\n",
    "            return df.to_parquet(f, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is \"parquet\"?\n",
    "\n",
    "[Parquet](https://parquet.apache.org/) is an efficient file format that is very good for **columnar** data storage, think metadata in [tidy data](http://vita.had.co.nz/papers/tidy-data.html) style. \n",
    "\n",
    "It's NOT good for storing big, sparse count matrices like gene expression data - use [hdf5](http://docs.h5py.org/en/stable/) for that. Pandas can directly [read/write hdf5](https://pandas.pydata.org/pandas-docs/stable/io.html#hdf5) and [xarray](http://xarray.pydata.org/en/stable/) is a great library for working with tons of labeled, indexed sparse matrices.\n",
    "\n",
    "Let's take a look at how quickly the search results file gets written in csv vs parquet format, using the `%time` magic to capture the time it takes for a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3_utils\n",
    "\n",
    "fmt = 'csv'\n",
    "s3_output_prefix = 's3://olgabot-maca/facs/lung_cancer_v4_metadata'\n",
    "\n",
    "%time s3_utils.write_s3(search_results_metadata, f'{s3_output_prefix}.{fmt}', fmt=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = 'parquet'\n",
    "\n",
    "%time s3_utils.write_s3(search_results_metadata, f'{s3_output_prefix}.{fmt}', fmt=fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the file size difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE sourmash/\r\n",
      "                           PRE sourmash_compare/\r\n",
      "                           PRE sourmash_compare_combined/\r\n",
      "                           PRE sourmash_compare_no_track_abundance/\r\n",
      "                           PRE sourmash_compare_no_track_abundance_combined/\r\n",
      "                           PRE sourmash_compute_all/\r\n",
      "                           PRE sourmash_compute_all_b_cells/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=100/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1000/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1100/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1200/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1300/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1400/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1500/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1600/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1700/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1800/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=1900/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=200/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=2000/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=2500/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=300/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=3000/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=3500/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=400/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=4000/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=4500/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=500/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=5000/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=600/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=700/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=800/\r\n",
      "                           PRE sourmash_dna-only_trim=false_scaled=900/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=100/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1000/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1000_test_subset/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1100/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1200/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1300/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1400/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1500/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1600/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1700/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1800/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=1900/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=200/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=2000/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=2500/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=300/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=3000/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=3500/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=400/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=4000/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=4500/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=500/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=5000/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=600/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=700/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=800/\r\n",
      "                           PRE sourmash_dna-only_trim=true_scaled=900/\r\n",
      "                           PRE sourmash_failed_samples/\r\n",
      "                           PRE sourmash_index/\r\n",
      "                           PRE sourmash_index_all/\r\n",
      "                           PRE sourmash_index_test/\r\n",
      "                           PRE sourmash_search/\r\n",
      "                           PRE sourmash_trim=true_scaled=100/\r\n",
      "2018-06-11 11:16:08  127.6 MiB aws_maca_remux.txt\r\n",
      "2018-06-11 11:16:08  127.6 MiB aws_maca_remux_bytes.txt\r\n",
      "2018-06-11 11:16:11   37.6 MiB aws_maca_remux_bytes_fastq.txt\r\n",
      "2018-06-11 11:16:08   37.6 MiB aws_maca_remux_fastq.txt\r\n",
      "2018-06-11 11:16:08   16.1 MiB fastqs.csv\r\n",
      "2018-06-11 11:16:10   21.2 MiB fastqs_sourmash.csv\r\n",
      "2018-10-16 10:55:39  433.8 MiB lung_cancer_v4_metadata.csv\r\n",
      "2018-10-16 10:56:57  101.7 MiB lung_cancer_v4_metadata.parquet\r\n",
      "2018-10-01 11:21:23   46.9 MiB signature-summaries.csv\r\n",
      "2018-09-24 04:52:33   30.4 MiB sourmash_compute_cpu_mem_time_profile.csv\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls --human-readable s3://olgabot-maca/facs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some plotting!!! \n",
    "\n",
    "Finally, let's make some pictures!\n",
    "\n",
    "Let's take a look at the overall distribution of the similarity scores using a [`kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) aka a smoothed histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(search_results_metadata)\n",
    "g.map(sns.kdeplot, 'similarity', shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is similarity different for different target Tabula Muris tissues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(search_results_metadata, hue='tissue')\n",
    "g.map(sns.kdeplot, 'similarity', shade=True)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is similarity different when `ignore_abundance` is True or False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(search_results_metadata, col='ignore_abundance', hue='tissue')\n",
    "g.map(sns.kdeplot, 'similarity', shade=True)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same plot, but made interactive with [`holoviews`](http://holoviews.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Distribution [filled=False, tools=['hover'] width=800 height=600] \n",
    "%%opts Distribution (line_color=Cycle(\"Category20\"))\n",
    "%%opts NdOverlay [legend_position='left']\n",
    "\n",
    "\n",
    "hmap = hv.HoloMap({ignore_abundance: hv.NdOverlay({name: hv.Distribution(df['similarity']) \n",
    "              for name, df in abundance_df.groupby('tissue')})\n",
    "                   for ignore_abundance, abundance_df \n",
    "                   in search_results_metadata.groupby('ignore_abundance')}, \n",
    "                  kdims=['ignore_abundance'])\n",
    "hmap.layout('ignore_abundance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about by cell ontology class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = sns.FacetGrid(search_results_metadata, col='ignore_abundance', hue='cell_ontology_class')\n",
    "g.map(sns.kdeplot, 'similarity', shade=True)\n",
    "g.add_legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the most commonly similar cell ontology class for the best 100 matches\n",
    "\n",
    "Best = highest similarity, so if we sort by the largest similarity score, we'll have the best!\n",
    "\n",
    "Use `mode` (remember mean, median and mode?) since we want the celltype that appears the most often in the closest 100 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = search_results_metadata.groupby(['query_sample_id', 'ignore_abundance'])\n",
    "\n",
    "cell_ontology_class_guess = grouped.apply( \n",
    "    # Need to force `astype(str)` otherwise get an error \n",
    "    # of comparing floats (NaNs) to strings\n",
    "    lambda x: x.nlargest(100, 'similarity')['cell_ontology_class'].mode())\n",
    "cell_ontology_class_guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the most common tissues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.apply(lambda x: x.nlargest(100, 'similarity')['tissue'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dataframe of the cell ontology class guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_ontology_class_guess_df = cell_ontology_class_guess.reset_index()\n",
    "cell_ontology_class_guess_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot the dataframe so that \"ignore_abundance=True,False\" are the columns and the sampel ids are the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_ontology_class_guess_pivotted = cell_ontology_class_guess_df.pivot(\n",
    "    index='query_sample_id', columns='ignore_abundance', values=0)\n",
    "\n",
    "# Set everything as a string for now\n",
    "cell_ontology_class_guess_pivotted = cell_ontology_class_guess_pivotted.astype(str)\n",
    "cell_ontology_class_guess_pivotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cell_ontology_classes = sorted(np.unique(cell_ontology_class_guess_pivotted.astype(str).values.flat))\n",
    "sorted_cell_ontology_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "matrix = confusion_matrix(cell_ontology_class_guess_pivotted['True'], \n",
    "                          cell_ontology_class_guess_pivotted['False'],\n",
    "                         labels=sorted_cell_ontology_classes)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataframe because they are better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_df = pd.DataFrame(matrix, index=sorted_cell_ontology_classes, columns=sorted_cell_ontology_classes)\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_df, annot=True, cmap='Purples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's make some interactive plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Distribution [filled=False, tools=['hover'] width=800 height=600] \n",
    "%%opts Distribution (line_color=Cycle(\"Category20\"))\n",
    "%%opts NdOverlay [legend_position='right']\n",
    "\n",
    "\n",
    "hv.NdOverlay({name: hv.Distribution(df['similarity']) \n",
    "              for name, df in search_results_metadata.groupby('tissue')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help message from holoviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hv.help(hv.NdOverlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Distribution [filled=False, tools=['hover'] width=800 height=600] \n",
    "%%opts Distribution (line_color=Cycle(\"Category20\"))\n",
    "%%opts NdOverlay [legend_position='left']\n",
    "\n",
    "\n",
    "hmap = hv.HoloMap({ignore_abundance: hv.NdOverlay({name: hv.Distribution(df['similarity']) \n",
    "              for name, df in abundance_df.groupby('tissue')})\n",
    "                   for ignore_abundance, abundance_df \n",
    "                   in search_results_metadata.groupby('ignore_abundance')}, \n",
    "                  kdims=['ignore_abundance'])\n",
    "hmap.layout('ignore_abundance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Distribution [filled=False, tools=['hover'] width=800 height=600] \n",
    "%%opts Distribution (line_color=Cycle())\n",
    "%%opts NdOverlay [legend_position='right']\n",
    "\n",
    "groupby = 'cell_ontology_class'\n",
    "\n",
    "hv.NdOverlay({name: hv.Distribution(df['similarity']) \n",
    "              for name, df in search_results_metadata.groupby(groupby)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: make faceted plot where the facets are the \"ignore abundance\" column and the colors are the cell ontology classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at median similarity per cell ontology per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cell_ontology = search_results_metadata.groupby(\n",
    "    ['query_sample_id', \"cell_ontology_class\", \"ignore_abundance\"])['similarity'].median()\n",
    "median_cell_ontology = median_cell_ontology.unstack(level=0)\n",
    "# median_cell_ontology = median_cell_ontology.reset_index()\n",
    "\n",
    "# Set the ignore_abundance level as the outer level for easier subsetting\n",
    "median_cell_ontology = median_cell_ontology.swaplevel()\n",
    "\n",
    "# Sort the index\n",
    "median_cell_ontology = median_cell_ontology.sort_index()\n",
    "\n",
    "# median_cell_ontology = median_cell_ontology.T\n",
    "print(median_cell_ontology.shape)\n",
    "median_cell_ontology.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore abundance \"True\" and \"False\" are strings so can subset this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cell_ontology_yes_abundance = median_cell_ontology.loc[\"False\", :]\n",
    "median_cell_ontology_yes_abundance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cell_ontology_no_abundance = median_cell_ontology.loc[\"True\", :]\n",
    "median_cell_ontology_no_abundance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(median_cell_ontology_yes_abundance, vmin=0, vmax=1, cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(median_cell_ontology_no_abundance, vmin=0, cmap='Purples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good cell, bad cell\n",
    "\n",
    "### Good cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`I3_B003573_S63` seems like a \"good cell\" that's similar to a lot of things. Let's look at its similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cell = \"I3_B003573_S63\"\n",
    "\n",
    "search_results_metadata.query('query_sample_id == @good_cell').groupby('ignore_abundance').apply(lambda x: x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_samples.query('id == @good_cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at its file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls --human-readable s3://czbiohub-seqbot/fastqs/180516_A00111_0149_AH5CM2DSXX/rawdata/$good_cell/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`D1_B003125_S25` seems like a weird bad cell that's not similar to anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cell = \"D1_B003125_S25\"\n",
    "search_results_metadata.query('query_sample_id == @bad_cell').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3_folder = os.path.dirname(compute_samples.query('id == @bad_cell').read1.iloc[0])\n",
    "s3_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls --human-readable $s3_folder/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHOA that's really tiny. Maybe the \"good cells\" are really ones with deep sequencing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cell_ontology = median_cell_ontology.sort_values(by=median_cell_ontology.columns.tolist(), ascending=False)\n",
    "median_cell_ontology.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seems like the read depth has to do with the similarity here\n",
    "\n",
    "So let's read all the file sizes for `read1` of the sample id!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "read1s = compute_samples.query('id in @search_results_metadata.query_sample_id').read1.unique()\n",
    "# print(read1s)\n",
    "for read1 in read1s:\n",
    "    ! aws s3 ls --human-readable $read1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll do the same thing but save the output to a list called `lines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for read1 in read1s:\n",
    "    line = ! aws s3 ls $read1\n",
    "    lines.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read1_txt = '\\n'.join(lines)\n",
    "\n",
    "read1_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pretend that plain strings as \"files\" for pandas, we use `StringIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "read1_aws_s3_ls = pd.read_table(StringIO(read1_txt), delim_whitespace=True, header=None, \n",
    "                          names=['date', 'time', 'bytes', 'basename'])\n",
    "read1_aws_s3_ls['sample_id'] = read1_aws_s3_ls['basename'].str.split('_R1').str[0]\n",
    "read1_aws_s3_ls = read1_aws_s3_ls.set_index('sample_id')\n",
    "read1_aws_s3_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read1_sizes = read1_aws_s3_ls['bytes']\n",
    "read1_sizes = read1_sizes.sort_values()\n",
    "read1_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's plot the similarity scores, ordered by the read1 sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x='query_sample_id', y='similarity', hue='ignore_abundance',\n",
    "              order=read1_sizes.index, data=search_results_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely looks like there's a trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "g = sns.FacetGrid(search_results, hue='query_sample_id', \n",
    "                  col='ignore_abundance', sharex=False, sharey=False,\n",
    "                  hue_order=read1_sizes.index, palette='viridis_r', size=2)\n",
    "g.map(sns.kdeplot, 'similarity', shade=True)\n",
    "g.savefig('ashley_lung_cancer_similarity_distribution_per_cell_showing_seq_depth.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commit the changes in a branch so you can see them after your instance is gone\n",
    "\n",
    "This is also helpful so you can just go to the github website for your branch and look at that for reference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To save your changes for the future, create a branch and commit your changes\n",
    "\n",
    "Since you are saving the output to YOUR own bucket, you'll want to make sure you have the code that made these changes, and the best way to do that is to use `git`.\n",
    "\n",
    "Create a branch named like this: `yourgithubsername/bioinformagician-part1`, e.g.:\n",
    "\n",
    "```\n",
    "git checkout -b olgabot/bioinformagician-part1\n",
    "```\n",
    "\n",
    "Add all the files in the `olgas_bioinformagician_tricks` folder:\n",
    "\n",
    "```\n",
    "cd ~/code/cupcakes/2018/\n",
    "git add -A olgas_bioinformagician_tricks\n",
    "```\n",
    "\n",
    "Write a message about what files you're committing and why:\n",
    "\n",
    "```\n",
    "git commit -m \"Use a s3 bucket I can write to\"\n",
    "```\n",
    "\n",
    "Try to push the changes:\n",
    "\n",
    "```\n",
    "git push\n",
    "```\n",
    "\n",
    "Then you'll get a \"fatal error\" (but really nobody died so why the freakout?) that looks like this:\n",
    "\n",
    "\n",
    "```\n",
    "fatal: The current branch olgabot/enable_quality_filtering has no upstream branch.\n",
    "To push the current branch and set the remote as upstream, use\n",
    "\n",
    "    git push --set-upstream origin olgabot/enable_quality_filtering\n",
    "```\n",
    "\n",
    "\n",
    "Copy/paste THEIR `git push` command which will properly link up your own branch name with the remote branch name on GitHub. (This is what I always do... I'm too lazy to write out the full command myself)\n",
    "\n",
    "[Check out your branch in the whole tree here!](https://github.com/czbiohub/cupcakes/network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bioinformagician]",
   "language": "python",
   "name": "conda-env-bioinformagician-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
